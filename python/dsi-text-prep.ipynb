{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic text prep with Python\n",
    "by Alex Provo and Jay Brodeur for [ARL DSI 2021](https://jasonbrodeur.github.io/dsi-text-prep/)\n",
    "\n",
    "In this notebook, we'll run through a few examples that introduce you to text exploration, manipulation, and transformation using Python. The purpose of these exercises is to familiarize you with basic concepts of preparing text in Python, and hopefully, encourage you to explore it a bit more. \n",
    "  \n",
    "Throughout this notebook, we've tried to document as much as possible the operations that we're performing, and the motivations behind doing so. More than anything, we're trying to convey the exploratory and often iterative nature of developing scripts and functions to prepare text for further analysis.  \n",
    "  \n",
    "Towards the end, we'll demonstrate some of the interesting things you can do with Natural Language Processing (NLP) packages like [NLTK](https://www.nltk.org/) and [spaCy](https://spacy.io/). There are **many** more tools and resources to explore, and we've listed some of them in the [Learn More](https://jasonbrodeur.github.io/dsi-text-prep/learn-more.html) page of the workshop website. \n",
    "\n",
    "We hope you enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will continue working with the article by Bernhard Berenson that was used in the [Open Refine](https://jasonbrodeur.github.io/dsi-text-prep/open-refine.html) exercises. \n",
    "\n",
    "**Citation:** Berenson, B. (1892). Some Comments on Correggio in Connection with His Pictures in Dresden. The Knight Errant, 1(3), 73-85. doi:10.2307/25515893\n",
    "  \n",
    "Our goals with the text are: \n",
    "* Identify and remove or fix errors--ideally, using repeatable methods that can be used on this text and perhaps many others in the corpus\n",
    "* Reduce the words in the text that have little value (very common 'stop words' like 'a', 'an, 'me', 'my', 'myself', 'we', 'our', 'ours', etc.) to improve the information content for analyses\n",
    "* Perform some analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and inspect\n",
    "First, let's load the file and preview the contents of our file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file\n",
    "with open('careggio-raw.txt') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "# Print the text to the screen\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a few minutes to inspect the text. What do you notice? \n",
    "* Are there obvious OCR errors present in the text? What might have caused these issues? \n",
    "* Are there other, less egregious spelling errors? \n",
    "* Are there patterns or similarities to the errors? \n",
    "* Is there extraneous information included in the text (e.g. page numbers, footnotes, etc.)?\n",
    "\n",
    "## 2. Tokenization\n",
    "Let's explore a few ways that we might address these issues. Our first step is to tokenize the text--which is to split the text into individual words that we can use in further preprocessing and analyses. \n",
    "\n",
    "Here, we will split the text by blank space (a decent place to begin with English text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into words by whitespace for a quick inspection:\n",
    "words = text.split()\n",
    "orig_num_words = len(words) # count the original number of words in the text.\n",
    "# Print a statement the communicates the total number of tokens:\n",
    "print(\"number of words: \" + str(len(words)))\n",
    "# Print the first 100 words for inspection: \n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Removing 'garbage' text\n",
    "One type of error that stands out are the 'garbage' words that contain multiple carets ```'^'```.  \n",
    "Let's identify all words that contain a caret to see if we can remove all of them to improve our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caret_words = list() # make a blank list that will hold our caret words.\n",
    "\n",
    "# Iterate through the entire list of words. If a '^' exists, add it to our 'caret_words' list.\n",
    "for i in words:\n",
    "    if '^' in i:\n",
    "        caret_words.append(i)   # Add to the list of caretted words\n",
    "# Print the list: \n",
    "print(\"Caretted words : \" + \"\\n\" + str(list(caret_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing the list, it seems as though making a rule that removes all words with a caret will mostly work, but we're going to end up also removing ```achieve^```, as well, which we would rather not do. So, we may want to try to find a rule that distinguishes the 'real' word from the 'garbage' ones.\n",
    "\n",
    "Most of the other garbage strings have either a capital letter or numeral in them. We can use a regular expression to find all strings that have a caret AND either a capital letter or numeral.\n",
    "\n",
    "To do this, we'll use [regular expressions](https://docs.python.org/3/howto/regex.html). There's a built-in module in Python called ```re``` that we can import to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the regular expression module\n",
    "import re \n",
    "\n",
    "# Iterate through the entire list of words. If a '^' exists in a word along with a capital letter \n",
    "# or a numeral, add it to our 'caret_words' list.\n",
    "caret_words = list() # make a blank list that will hold our caret words.\n",
    "\n",
    "for i in words:\n",
    "    a = re.search('[A-Z0-9]',i) # regular expression search for any capital letter or numeral\n",
    "    b = re.search('[\\^]',i)     # regular expression to search for a caret ('^') symbol\n",
    "    if a and b:                 # An if statement that is true if both regular expressions find a match\n",
    "        caret_words.append(i)   # if true, add to the list of caretted words to be removed\n",
    "        words.remove(i)         # if true, remove the matching words from our list\n",
    "        \n",
    "# Print out a list of the offending words:\n",
    "print(\"Caretted + capital/numeral words: \" +\"\\n\" + str(list(caret_words)))\n",
    "print(\"Original number of words: \" + str(orig_num_words) + \"; Number remaining: \" + str(len(words)))\n",
    "\n",
    "# Print the first 50 words to inspect: \n",
    "print(\"\\n\" + \"First 50 words: \" + \"\\n\" + str(words[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using a spellchecker\n",
    "As mentioned in the instroduction, there are A LOT of python packages available for various NLP operations. For this example, let's use a relatively simple one called ```pyspellcheck``` . This package has been pre-installed for you in this Notebook. If you were using a regular Python installation, you could install it with the command: ```pip install pyspellchecker```. Note that the [pyspellcheck website](https://pyspellchecker.readthedocs.io/)--as is the case for most packages--has excellent documentation about how to use it. \n",
    "\n",
    "In the example below, let's see if we can use it to help us identify misspelled words in our text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the SpellChecker module from the pyspellchecker package\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker() # initialize SpellChecker as an object\n",
    "\n",
    "# run SpellChecker on the first 100 items in our list of words and return only misspelled\n",
    "misspelled = spell.unknown(words[:100])\n",
    "\n",
    "# print the list of mispelled items from the first 100 words:\n",
    "print(\"misspelled words: \" + str(list(misspelled)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the list? It caught some obvious mispellings (like correg, ncrftjh), but has a number of false positives (like 'night.' ,'gallery.', 'ago,')?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Removing punctuation | Converting to lowercase\n",
    "Clearly, we need to get rid of the punctuation before we do any more processing, as it is causing correctly-spelled words to be falsely identified. This is an excellent illustration of the ways in which programs can be very literal and inflexible. It also underscores the importance of operations like punctuation removal before persuing higher-order NLP processes.  \n",
    "\n",
    "It's also a good time to normalize our case by reducing everything to lowercase. You might have noticed that the spellchecker is doing this for us before checking the spelling. Doing this may not be appropriate for certain approaches (like identifying proper nouns), but it will be necessary if we want to use the list of misspelled words to correct or remove errors from our text. \n",
    "\n",
    "To filter out punctuation, let's use the built-in list of punctuation from the 'string' package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the built-in list of punctuation from the 'string package'\n",
    "import string\n",
    "# Print the list of punctuation to the screen\n",
    "print(string.punctuation)\n",
    "\n",
    "# First, let's merge our list of words back into a single block of text: \n",
    "text_rejoined = \" \".join(words)\n",
    "\n",
    "# Remove punctuation with a regular expression: \n",
    "words = re.split(r'\\W+', text_rejoined)\n",
    "\n",
    "# Print the first 50 words to inspect: \n",
    "print(\"\\n\" + \"First 50 words: \" + \"\\n\" + str(words[:50]))\n",
    "\n",
    "# Print an update on number of words remaining\n",
    "print(\"Original number of words: \" + str(orig_num_words) + \"; Number remaining: \" + str(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that the number of words in our list is now greater than our starting number. This is a result of splitting by all punctuation--any contracted or hyphenated word has now been split into two. This is not an issue, as we'll remove these non-words later.\n",
    "\n",
    "Let's now convert our words to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conver to lowercase with the 'lower' function\n",
    "words = [i.lower() for i in words]\n",
    "\n",
    "# Print the first 50 words to inspect: \n",
    "print(\"\\n\" + \"First 50 words: \" + \"\\n\" + str(words[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the spellchecker once more. And this time, we'll also use some additional functions within the SpellChecker module (including suggested corrections and adding words to the dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run SpellChecker on the first 100 items in our list of words and return only misspelled\n",
    "misspelled = spell.unknown(words[:100])\n",
    "\n",
    "# print the list of mispelled items from the first 100 words:\n",
    "print(\"misspelled words: \" + str(list(misspelled)))\n",
    "\n",
    "# Print out a list of suggested corrected spellings:\n",
    "for i in misspelled:\n",
    "    print(\"original: \" + i + \"; suggested: \" + spell.correction(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the false positives are proper nouns like \"correggio\", \"sisto\", and \"sixtus\". We can add these proper nouns to our dictionary, so they are no longer flagged by our spell checker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to dictionary the 'mispelled' names that are not such:\n",
    "spell.word_frequency.load_words(['correggio', 'sixtus', 'sisto'])\n",
    "\n",
    "# Rerun the spellcheck:\n",
    "# run SpellChecker on the first 100 items in our list of words and return only misspelled\n",
    "misspelled = spell.unknown(words[:100])\n",
    "\n",
    "# print the list of mispelled items from the first 100 words:\n",
    "print(\"misspelled words: \" + str(list(misspelled)))\n",
    "\n",
    "\n",
    "# This would run the spellcheck and remove all erroneous words (NOT USED)\n",
    "# misspelled = spell.unknown(words)\n",
    "# for i in mispelled:\n",
    "#     words.remove(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Better! If we had more time, we could explore the rest of the text and make an educated decision whether it was better to keep the misspelled words, or remove them (alongside a few false positives). We could also build a list of proper nouns that could be permanently added to a dictionary we could use for a related corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Removing stop words with NLTK | Putting it all together\n",
    "[NLTK](https://www.nltk.org/) is a common and very powerful NLP package that can perform any number of operations in an NLP \"Pipeline\". In this example, we're going to start with our original text and repeat much of our previous processes using the NLTK toolkit. We'll also remove all 'stop words' (like 'a', 'an, 'me', 'my', 'myself', 'we', 'our', 'ours', etc.) from our file, and filter out those pesky single-character words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import the word tokenizer and stop words from nltk\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "### Read the text file\n",
    "with open('careggio-raw.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "### Here we will regenerate tokens from our original text. We'll call them tokens\n",
    "tokens_list = word_tokenize(text)\n",
    "print(\"\\n\" + \"First 50 tokens: \" + \"\\n\" + str(tokens_list[:50]))\n",
    "orig_num_words = len(tokens_list) # count the original number of words in the text.\n",
    "\n",
    "##########################\n",
    "### Insert our filtering for caretted words: \n",
    "caret_words = list() # make a blank list that will hold our caret words.\n",
    "\n",
    "# Iterate through the entire list of words. If a '^' exists, add it to our 'caret_words' list.\n",
    "for i in tokens_list:\n",
    "    a = re.search('[A-Z0-9]',i) # regular expression search for any capital letter or numeral\n",
    "    b = re.search('[\\^]',i)     # regular expression to search for a caret ('^') symbol\n",
    "    if a and b:                 # An if statement that is true if both regular expressions find a match\n",
    "        caret_words.append(i)   # if true, add to the list of caretted words to be removed\n",
    "        tokens_list.remove(i)         # if true, remove the matching words from our list\n",
    "\n",
    "        # Print the list: \n",
    "print(\"Caretted words removed : \" + \"\\n\" + str(list(caret_words)))\n",
    "\n",
    "### convert to lower case\n",
    "tokens_list = [i.lower() for i in tokens_list]\n",
    "\n",
    "##########################\n",
    "### Remove punctuation by making a replacment table between pucntuation and empty spaces\n",
    "repl_table = str.maketrans('', '', string.punctuation)\n",
    "tokens_nopunct = [i.translate(repl_table) for i in tokens_list]\n",
    "print(\"\\n\" + \"First 50 words after removing punctuation: \" + \"\\n\" + str(tokens_nopunct[:50]))\n",
    "\n",
    "##############################\n",
    "# Get rid of any other non-alphanumeric characters\n",
    "words = [word for word in tokens_nopunct if word.isalpha()]\n",
    "\n",
    "print(\"\\n\" + \"First 50 words after removing non-alphanumeric: \" + \"\\n\" + str(words[:50]))\n",
    "\n",
    "##############################\n",
    "# Remove stopwords with nltk\n",
    "words = [i for i in words if not i in stop_words]\n",
    "print(\"\\n\" + \"First 50 words after removing stop words: \" + \"\\n\" + str(words[:50]))\n",
    "\n",
    "# Print an update on number of words remaining\n",
    "print(\"Original number of words: \" + str(orig_num_words) + \"; Number remaining: \" + str(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! This final set is significantly reduced and much more normalized than before. There are still issues with erroneous words, and we would have to make a decision whether to further process the file to remove them, or if our analyses can tolerate a bit of inconsistency. \n",
    "  \n",
    "But, we're now at the point where we can perform some initial analyses on the data. For example, let's list the 20 most common words in the text:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pandas package\n",
    "import pandas                       \n",
    "from collections import Counter\n",
    "\n",
    "# Take the word count using the Counter function\n",
    "word_counts = Counter(words)\n",
    "# Print it to the screen\n",
    "print(\"Most common words and frequencies:\")\n",
    "word_counts.most_common(20)\n",
    "#type(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. A little bit more fun\n",
    "Finally, let's use [spaCy](https://spacy.io/)--another very powerful and full-featured NLP package--to see some of the higher-order analyses that such packages can perform. \n",
    "\n",
    "Here, we're going to use our text file to do some parts-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the text file\n",
    "with open('careggio-raw.txt') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "# Import spacy and load the small pipeline\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Run our text against the model\n",
    "doc = nlp(text)\n",
    "\n",
    "# Use the displacy module to display parts of speech\n",
    "from spacy import displacy\n",
    "displacy.render(doc,style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrap-up\n",
    "Although these exercises provided only the most preliminary introduction to text preparation for analysis and dissemination, we hope that it gave you a sense of the broad potential for using programmatic approaches to do so. To explore some next steps, refer to the [Learn More](https://jasonbrodeur.github.io/dsi-text-prep/learn-more.html) section of the workshop website. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
